​                          AI 为什么这么贵？

本文引自https://www.bloomberg.com/news/articles/2024-04-30/why-artificial-intelligence-is-so-expensive?srnd=phx-ai&leadSource=uverify%20wall 彭博社的一篇报道。



4月25日，Microsoft 表示，它在最近一个季度的资本支出上花费了140亿美元，并预计这些成本将“大幅增加”，部分原因是 AI 基础设施投资。这比去年同期增长了 <u>79%</u>。Alphabet 表示，它在本季度花费了 <u>120 亿</u>美元，比去年同期增长了 91%，并预计今年剩余时间将“达到或高于”这一水平，因为它专注于人工智能机会。与此同时，Meta 上调了对今年投资的估计，现在认为资本支出将为 350 亿至 400 亿美元，这将在该区间的高端增长 42%。它列举了对人工智能研究和产品开发的积极投资。



## 大语言模型参数规模更大

   当今最著名的 AI 产品，包括 OpenAI 的 ChatGPT，都由大型语言模型提供支持——这些系统被输入大量数据，包括书籍、文章和在线评论，以便对用户的查询做出最佳回应。许多领先的 AI 公司都在押注，通往更复杂的人工智能（甚至可能是在许多任务上都能胜过人类的 AI 系统）的道路是让这些大型语言模型变得更大。

​    <u>这需要采购更多的数据、更多的计算能力，并且还需要更长时间地训练 AI 系统</u>。

4 月初，OpenAI 的竞争对手 Anthropic 的首席执行官 Dario Amodei 表示，目前市场上的 AI 模型的训练成本约为 1 亿美元。

“目前正在训练的模型以及将在今年晚些时候或明年年初的不同时间推出的模型的成本接近 10 亿美元，”他说。“然后我认为在 2025 年和 2026 年，我们将更多地达到 5 美元或 100 亿美元。”

## 芯片和计算成本

​    其中大部分成本与芯片有关。这些不是让 [Intel Corp.](https://www.bloomberg.com/quote/INTC:undefined) 闻名的中央处理器 （CPU），也不是为数十亿部智能手机提供动力的精简版移动表亲。为了训练大型语言模型，AI 公司依靠图形处理单元 （GPU） 来高速处理大量数据。这些芯片不仅供不应求，而且价格非常昂贵，最尖端的功能主要由一家公司提供：Nvidia。

   Nvidia 的 H100 图形芯片是训练 AI 模型的黄金标准，售价估计为 30,000 美元，一些经销商以该价格的数倍的价格出售。大型科技公司需要大量的 TOOLS。Meta 首席执行官扎克伯格[此前表示](https://www.instagram.com/reel/C2QARHJR1sZ/?hl=en)，他的公司计划在今年年底前采购 350,000 颗 H100 芯片，以支持其 AI 研究工作。即使他获得了批量购买折扣，那也很容易加起来达到数十亿美元。

   公司可以在不购买实际芯片的情况下完成这项工作，但借用芯片也很昂贵。换言之：Amazon.com Inc. 的云部门将以每小时约 6 美元的价格向客户租用一个由 Intel 构建的大型主力处理器集群。相比之下，一组 Nvidia H100 芯片每小时的成本接近 100 美元。

   上个月，Nvidia 推出了一款名为 Blackwell 的新处理器设计，该处理器在处理大型语言模型时速度提高了数倍，预计其定价将与包括 H100 在内的 Hopper 系列相似。Nvidia 表示，大约需要 [2000](https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai) 个 Blackwell GPU 来训练一个 1.8 万亿参数的 AI 模型。根据《纽约时报》对 OpenAI 的 GPT-4 提起的诉讼，该诉讼是该初创公司使用其文章来训练 AI 系统的。相比之下，Nvidia 表示需要 8,000 个 Hopper GPU 来执行相同的任务。但这种改进可能会被行业推动构建更大的 AI 模型所抵消。





## 数据中心

   购买这些芯片的公司需要找个地方来放置它们。Meta 与最大的云计算公司——亚马逊、Microsoft 和 Google——以及其他计算能力出租提供商正在竞相构建新的服务器场。这些建筑往往是定制的。它们容纳着硬盘驱动器、处理器、冷却系统以及成堆的电气设备和备用发电机。

​    研究人员 Dell'Oro Group 估计，公司今年将花费 2940 亿美元建造和装备数据中心，高于 2020 年的 1930 亿美元。这种扩张在很大程度上与数字服务的广泛增长有关，包括流媒体视频、企业数据的爆炸式增长、您的社交媒体源。但越来越多的支出被指定用于昂贵的 Nvidia 芯片和支持 AI 繁荣所需的其他专用硬件。

   根据市场情报公司 DC Byte 的数据，全球现在有 [7,000 多个数据中心](https://www.bloomberg.com/graphics/2024-ai-data-centers-power-grids/)，包括处于不同发展阶段的设施，而 2015 年为 3,600 个。这些设施也越来越大。根据 DC Byte 的数据，全球数据中心建筑的平均面积现在为 412,000 平方英尺，自 2010 年以来几乎增长了五倍。



## 内容版权和人才

[几家欧洲出版商](https://www.bloomberg.com/news/articles/2024-03-13/openai-inks-deals-with-european-publishers-le-monde-prisa?srnd=technology-ai)将他们的新闻内容整合到 ChatGPT 中，并训练其 AI 模型。这些交易的财务条款尚未披露，但彭博新闻社此前曾报道称，OpenAI [同意向](https://www.bloomberg.com/news/articles/2023-12-13/openai-axel-springer-ink-deal-to-use-news-content-in-chatgpt) Politico 和 Business Insider 的德国出版商 Axel Springer SE 支付数千万欧元，以获得其新闻文章的使用权。这家初创公司还与《时代周刊》、CNN 和 Fox News [进行了谈判](https://www.bloomberg.com/news/articles/2024-01-10/openai-in-talks-with-cnn-fox-and-time-to-license-content)，以获得内容许可。

​    虽然 OpenAI 在获得许可交易方面更加积极，但大型科技公司也在寻找方法来获取构建引人注目的 AI 工具所需的语言数据。Google 已经[达成了一项价值 6000 万美元](https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/)的交易，从 Reddit 获得数据许可。据报道，Meta的员工[正在讨论](https://www.nytimes.com/2024/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html)收购图书出版商Simon & Schuster，纽约时报报道。

   科技公司也陷入了一场争夺 AI 人才的激烈战争。去年的某个时候，[Netflix 公司](https://www.bloomberg.com/quote/NFLX:undefined)招聘 [AI 产品经理职位](https://www.cbsnews.com/news/netflix-ai-hiring-product-manager-900000/)，薪水高达 900,000 美元。

## **更便宜的替代品**

 Microsoft 在推动大型语言模型的狂热方面做得比大多数公司都多，它最近表示将尝试一种不同的方法。该公司取笑了三种计算密集度较低的[小型 AI 模型](https://www.nytimes.com/2024/04/23/technology/microsoft-ai.html)。

  Microsoft 表示，大型语言模型“仍将是解决许多类型复杂任务的黄金标准”，例如“高级推理、数据分析和上下文理解”。但较小的模型可能足以满足某些客户和使用案例的需求。其他公司，包括由两名前 Google 员工创立的初创公司 [Sakana AI](https://www.bloomberg.com/news/articles/2024-01-16/ai-startup-sakana-raises-30-million-to-build-smaller-ai-models) 也专注于小型模型。

  “你不需要一直都需要一辆跑车，”Forrester Research 专注于 AI 的高级分析师 Rowan Curran 说。 “有时你需要一辆小型货车或一辆皮卡车。它不会是每个人都用于所有用例的一大类模型。

  然而，就目前而言，AI 领域的传统智慧是越大越好。那将是昂贵的。